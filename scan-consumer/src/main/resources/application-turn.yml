spring:
  application:
    name: scan-consumer
  #Data source configuration
  datasource:
    type: com.alibaba.druid.pool.DruidDataSource
    driver-class-name: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://192.168.31.114:12012/scan_${profile}?useUnicode=true&characterEncoding=utf-8&useSSL=false&useTimezone=true&serverTimezone=GMT%2B8&allowMultiQueries=true
    username: qiliu
    password: qiliu123456
    druid:
      # Configuration information of connection pool
      #Initialize size, minimum, maximum
      initial-size: 5
      min-idle: 5
      maxActive: 20
      # Configure the timeout to wait for a connection
      maxWait: 60000
      # Configure the interval between detection and detection of idle connections that need to be closed. The unit is milliseconds.
      timeBetweenEvictionRunsMillis: 60000
      # Configure the minimum survival time of a connection in the pool, in milliseconds
      minEvictableIdleTimeMillis: 300000
      validationQuery: SELECT 1 FROM DUAL
      testWhileIdle: true
      testOnBorrow: false
      testOnReturn: false
      # Turn on PSCache and specify the size of PSCache on each connection
      poolPreparedStatements: true
      maxPoolPreparedStatementPerConnectionSize: 20
      # Configure the filters for monitoring and statistics interception. After removing them, the monitoring interface SQL cannot collect statistics. 'wall' is used for firewalls.
      filters: stat
      filter:
        config: true
        # Enable status monitoring of druiddatasource
        stat:
          enabled: true
          db-type: mysql
          # Enable slow sql monitoring. If it exceeds 2 seconds, it will be considered as slow sql and recorded in the log.
          log-slow-sql: true
          slow-sql-millis: 500
          merge-sql: true
      # Configure DruidStatFilter
      web-stat-filter:
        enabled: true
        url-pattern: /*
        exclusions: '*.js,*.gif,*.jpg,*.bmp,*.png,*.css,*.ico,/druid/*'
      # Configure DruidStatViewServlet
      stat-view-servlet:
        # Whether to enable StatViewServlet (monitoring page), the default value is false (considering security issues, it is not enabled by default. If you need to enable it, it is recommended to set a password or whitelist to ensure security)
        enabled: true
        url-pattern: /druid/*
        # IP whitelist (if not configured or empty, all access is allowed)
        # allow: 127.0.0.1,192.168.46.120
        # IP blacklist (when common, deny takes precedence over allow)
        # deny: 192.168.46.121
        # Disable the "Reset All" function on HTML pages
        reset-enable: false
        # log-in name
        login-username: admin
        # login password
        login-password: 123456
        allow: ""
        deny: ""

  #Redis key configuration
  redis:
    password: "ffca@sn!u934"
    database: 0
    cluster:
      max-redirects: 3
      nodes: 192.168.31.115:6381,192.168.31.115:6382,192.168.31.115:6383,192.168.31.115:6384,192.168.31.115:6385,192.168.31.115:6386
    key:
      #How many pieces of data can be stored at most?
      max-item: 500000
      #Block list
      blocks: ${turn.redis-namespace}:blocks
      #Transaction list
      transactions: ${turn.redis-namespace}:transactions
      #Statistics
      networkStat: ${turn.redis-namespace}:networkStat
      #Internal transfer transaction
      transferTx: ${turn.redis-namespace}:transferTx
      #erc20trade
      erc20Tx: ${turn.redis-namespace}:erc20Tx
      #erc721 Transaction
      erc721Tx: ${turn.redis-namespace}:erc721Tx
      #erc1155 Transaction
      erc1155Tx: ${turn.redis-namespace}:erc1155Tx
      #Joined games list
      addrGames: ${turn.redis-namespace}:addrGames
      #bubble information
      bubbleInfo: ${turn.redis-namespace}:bubbleInfo
  #ES index configuration
  elasticsearch:
    high-level-client:
      hosts: 192.168.31.115:19200,192.168.31.115:29200,192.168.31.115:39200
      port: 19200
      schema: http
      username:
      password:
    index:
      blockIndexName: ${turn.es-namespace}_block
      transactionIndexName: ${turn.es-namespace}_transaction
      delegationIndexName: ${turn.es-namespace}_delegation
      nodeOptIndexName: ${turn.es-namespace}_nodeopt
      delegationRewardIndexName: ${turn.es-namespace}_delegation_reward
      transferTxIndexName: ${turn.es-namespace}_transfer_tx
      erc20TxIndexName: ${turn.es-namespace}_erc20_tx
      erc721TxIndexName: ${turn.es-namespace}_erc721_tx
      erc1155TxIndexName: ${turn.es-namespace}_erc1155_tx
      microNodeOptIndexName: ${turn.es-namespace}_micro_node_opt
      subChainTxIndexName: ${turn.es-namespace}_sub_chain_tx
scan:
  kafka:
    consumer:
      # Client ID
      client-id: scan-consumer
      # Kafka server
      bootstrap-servers: 192.168.31.115:19092,192.168.31.115:29092,192.168.31.115:39092
      #Consumer group ID
      group-id: scan-consumer-${spring.profiles.active}-consumer-group
      # Whether to automatically commit offsets. The default value is true. In order to avoid duplicate data and data loss, you can set it to false and then manually commit offsets.
      enable-auto-commit: false
      # This attribute specifies what the consumer should do when reading a partition without an offset or an invalid offset:
      # earliest: When there is a submitted offset under each partition, consumption starts from the submitted offset; when there is no submitted offset, consumption of partition records starts from the beginning.
      # latest: When there is a submitted offset under each partition, consumption starts from the submitted offset; when there is no submitted offset, the newly generated data under the partition is consumed (records generated after the consumer is started)
      # none: When each partition has a submitted offset, consumption starts from the submitted offset; as long as one partition does not have a submitted offset, an exception is thrown.
      auto-offset-reset: earliest
      # This parameter defines the maximum number of messages that the poll method can pull. The default value is 500. If there are less than 500 new messages when pulling messages, as many as there are will be returned; if there are more than 500, only 500 will be returned each time.
      # This default value is too large in some scenarios. In some scenarios, it is difficult to guarantee that 500 messages can be processed within 5 minutes.
      # If the consumer cannot process 500 messages within 5 minutes, reBalance will be triggered.
      # Then this batch of messages will be assigned to another consumer, and it will still be processed, so this batch of messages will never be processed.
      # To avoid the above problems, evaluate in advance the maximum time required to process a message, and then override the default max.poll.records parameter
      # Note: BatchListener needs to be turned on for batch monitoring to take effect. If BatchListener is not turned on, reBalance will not occur.
      max-poll-records: 5
      # Indicates that the Consumer will only read the messages written by the transactional Producer that successfully submitted the transaction. It can also see all the messages written by the non-transactional Producer.
      isolation-level: read_committed
      # Request timed out
      request-timeout-ms: 120000
      # When the broker does not receive the consumer's heartbeat request for a long time, reBalance is triggered. The default value is 10s.
      session-timeout-ms: 120000
      # The maximum interval between two polls, the default value is 5 minutes. If this interval is exceeded, reBalance will be triggered.
      listener-poll-timeout: 600000
      # When the topic monitored by the consumption listening interface does not exist, an error will be reported by default, so set it to false to ignore the error.
      listener-missing-topics-fatal: false
      #Set whether to consume in batches
      listener-batch-listener: false
      # When concurrency<the number of partitions, uneven consumption will occur. A consumer thread may consume data from multiple partitions.
      # When concurrency=the number of partitions, the best state is that a consumer thread consumes the data of one partition
      # When concurrency>the number of partitions, some consumer threads will have no consumable partitions, causing a waste of resources.
      #Total number of consumer threads =concurrency*number of machines, each @KafkaListener method will start 3 threads
      listener-concurrency: 3
      # Retry interval, in seconds
      retry-interval: 10000
      # Maximum number of retries
      retry-max-attempts: 3
    topic:
      sub-chain-tx: scan-${spring.profiles.active}-sub-chain-tx
# MyBatis configuration (note: must correspond to the path of the mapper mapping xml file)
mybatis:
  mapper-locations: classpath*:mapper/*.xml, classpath*:custommapper/*.xml
  # Entity category name configuration (note: the path to the corresponding entity class)
  type-aliases-package: com.turn.browser.dao.entity
profile: platon
# The size of the memory ring buffer must be an exponential multiple of 2
disruptor:
  queue:
    # Block event ring buffer size
    blockBufferSize: 1024
    #Collect event ring buffer size
    collectionBufferSize: 1024
    #Data supplement ring buffer size
    complementBufferSize: 1024
    # gas price estimates message ring buffer size
    gasEstimateBufferSize: 1024
    #Data persistence ring buffer size
    persistenceBufferSize: 1024
    #How many to process each time
    persistenceBatchSize: 10
    #Log configuration
logging:
    #Log level
  level:
    org.springframework: info
    org.springframework.retry: info
    com.turn.browser: info
    com.turn.browser.bootstrap: info
    com.turn.browser.collection: info
    com.turn.browser.collection.queue.handler: info
    com.turn.browser.common: info
    com.turn.browser.complement: info
    com.turn.browser.complement.handler: info
    com.turn.browser.complement.converter: info
    com.turn.browser.persistence: info
    com.turn.browser.persistence.handler: info
    com.turn.browser.persistence.queue.handler: info
    com.turn.browser.task: info
#Turn related configuration
turn:
  #Address HRP prefix
  addressPrefix: turn
  # Amount display unit
  valueUnit: turn
  # Number of parallel decoding threads for transaction input parameters
  txLogDecodeThreadNum: 200
  # Web3j access address
  web3j:
    #HTTPorWS
    protocol: WS
    addresses: 10.32.165.85:6790
  # How many consecutive periods can the statistical annualized rate of pledge nodes take at most?
  maxSettlePeriodCount4AnnualizedRateStat: 4
  #TURN Total Initial Circulation Volume (TURN)
  initIssueAmount: 10000000000
  #Fixed issuance ratio every year
  addIssueRate: 0.025
  #The proportion of additional issuance allocated to the incentive pool each year
  incentiveRateFromIssue: 0.8
  #How many blocks are rolled back in each consensus round is the time to elect the next round of validators
  electionBackwardBlockCount: 20
  #The amount of money the Calculator Foundation will fill into the incentive pool within 10 years: <year - subsidy amount (TURN)>
  foundationSubsidies: {
    '1':62215742.00000000,
    '2':55965742.00000000,
    '3':49559492.00000000,
    '4':42993086.00000000,
    '5':36262520.00000000,
    '6':29363689.00000000,
    '7':22292388.00000000,
    '8':15044304.00000000,
    '9':7615018.00000000
  }
  # Default pledge amount of initial built-in node (TURN)
  defaultStakingLockedAmount: 150000
  #Block maximum gas limit
  maxBlockGasLimit: 201600000
  #Proposal url parameter template
  proposalUrlTemplate: https://github.com/PlatONnetwork/PIPs/blob/master/PIPs/PIP-%s.md
  #Proposal pip_num parameter template
  proposalPipNumTemplate: PIP-%s
  #keysBaseURL
  keyBase: https://keybase.io/
  #keyBase api
  keyBaseApi: _/api/1.0/user/lookup.json?key_suffix=
  #chainid
  chainId: 100
  #Index query paging parameter configuration during synchronization
  paging:
    erc20-transaction:
      pageSize: 2000
      pageCount: 250
  #Redis namespace
  redis-namespace: browser:${profile}
  #Esindex namespace
  es-namespace: browser_${profile}
  #0 Loop access time for block waiting (seconds)
  zeroBlockNumber:
    wait-time: 60

v0150:
  #Effective version of the lock minimum release amount parameter
  restrictingMinimumReleaseActiveVersion: 3584
  #Adjust effective version
  adjustmentActiveVersion: 3840
  #Adjustment proposal ID
  adjustmentPipId: 14
  #Adjustment log output file
  adjustLogFilePath: ./logs/staking-delegate-adjust.log
task:
  #Address statistics task batch size
  addressBatchSize: 1000
  #The adjustment operation is triggered when agent differs by how many block numbers from the actual block number on the chain
  gapForAdjust: 20
  #Unable to catch up with the chain, cache size of ES and Redis in batches
  esRedisNotCatchupBatchSize: 10
  #Has caught up with the chain, batch into the cache size of ES and Redis
  esRedisCatchupBatchSize: 1
xxl:
  job:
    admin:
      #Deployment and address of dispatch center
      addresses: http://192.168.31.110:18080/xxl-job-admin
    #Dispatch center communication TOKEN
    accessToken: qiliu123456
    #Number of days to save dispatch center log files
    logretentiondays: 90
    executor:
      #ExecutorAppName
      appname: turn-scan-agent
      IP: 192.168.31.212
      #Executor port number
      port: 9999
      #Executor running log file storage disk path
      logpath: ./logs/jobhandler
      #Number of days to save executor log files
      logretentiondays: 90
apollo:
  bootstrap:
    # Whether to enable Apollo
    enabled: false
#Performance configuration
server:
  port: 40040
  tomcat:
    max-connections: 20000
    threads:
      max: 2000
      min-spare: 800
  servlet:
    context-path: /browser-consumer
